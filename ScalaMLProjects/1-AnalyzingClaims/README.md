# 1. Analyzing Insurance Severity Claims

Chapter 1, Analyzing Insurance Severity Claims, shows how to develop a predictive model
for analyzing insurance severity claims using some widely used regression techniques. We
will demonstrate how to deploy this model in a production-ready environment.

# Exploratory analysis of the dataset: EDA.scala file

The dataset can be downloaded in: https://www.kaggle.com/c/allstate-claims-severity/data

In this file we can see:

- how can structure a Spark.scala code

- Read data

- Analyse data with SQL queries and Spark.


# Preprocessing : Preprocessing.scala file

In this file we can see how

- clean rows with null values in columns

- split data into training and validation

- construct data set to model 

  - select only features columns
  
  - use StringIndexer to encode a given string column of labels to a column of label indices in categorical feature.
  
    - OneHotEncoder maps a column of label indices to a column of binary
      vectors, with a single one-value at most. This encoding permits algorithms
      that expect continuous features, such as logistic regression, to utilize
      categorical features.

- transform a list of columns in a vector

  - VectorAssembler is a transformer. It combines a given list of columns
    into a single vector column. It is useful for combining the raw features and
    features generated by different feature transformers into one feature
    vector, in order to train ML models such as logistic regression and
    decision trees.
    
 **ML: Linear Regression: for predicting insurance severity claims**
 
The goal of regression is to find relationships and dependencies between variables. It models the
relationship between a continuous scalar dependent variable _y_ (that is, label or target) and
one or more (a D-dimensional vector) explanatory variable (also independent variables,
input variables, features, observed data, observations, attributes, dimensions, and data
points) denoted as _x_ using a linear function. 

Note that there are no single **metrics** in terms of regression errors; there are several as follows:

- Mean Squared Error (MSE): It is a measure of how close a fitted line is to data
points. The smaller the MSE, the closer the fit is to the data.
Root Mean Squared Error (RMSE): It is the square root of the MSE but probably
the most easily interpreted statistic, since it has the same units as the quantity
plotted on the vertical axis.

- R-squared: R-squared is a statistical measure of how close the data is to the fitted
regression line. R-squared is always between 0 and 100%. The higher the Rsquared,
the better the model fits your data.

- Mean Absolute Error (MAE): MAE measures the average magnitude of the
errors in a set of predictions without considering their direction. It's the average
over the test sample of the absolute differences between prediction and actual
observation where all individual differences have equal weight.

- Explained variance: In statistics, explained variation measures the proportion to
which a mathematical model accounts for the variation of a given dataset.


**Spark ML pipelines** have the following components:

- DataFrame: Used as the central data store where all the original
data and intermediate results are stored.

- Transformer: A transformer transforms one DataFrame into
another by adding additional feature columns. Transformers are
stateless, meaning that they don't have any internal memory
and behave exactly the same each time they are used.

- Estimator: An estimator is some sort of ML model. In contrast to
a transformer, an estimator contains an internal state
representation and is highly dependent on the history of the
data that it has already seen.

- Pipeline: Chains the preceding components, DataFrame,
Transformer, and Estimator together.

- Parameter: ML algorithms have many knobs to tweak. These
are called hyperparameters, and the values learned by a ML
algorithm to fit data are called parameters.

