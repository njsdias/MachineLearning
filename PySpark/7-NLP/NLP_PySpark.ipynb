{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP \n",
    "    \n",
    "### 1- Tokenize & StopWords \n",
    "### 2- countvectorizer for feature vectorization\n",
    "### 3- VectorAssembler\n",
    "### 4- Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "#import SparkSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame([(1,'I really liked this movie'),\n",
    "                         (2,'I would recommend this movie to my friends'),\n",
    "                         (3,'movie was alright but acting was horrible'),\n",
    "                         (4,'I am never watching that movie ever again')],\n",
    "                        ['user_id','review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+\n",
      "|user_id|review                                    |\n",
      "+-------+------------------------------------------+\n",
      "|1      |I really liked this movie                 |\n",
      "|2      |I would recommend this movie to my friends|\n",
      "|3      |movie was alright but acting was horrible |\n",
      "|4      |I am never watching that movie ever again |\n",
      "+-------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='review',outputCol='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df=tokenization.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|user_id|review                                    |tokens                                             |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|1      |I really liked this movie                 |[i, really, liked, this, movie]                    |\n",
      "|2      |I would recommend this movie to my friends|[i, would, recommend, this, movie, to, my, friends]|\n",
      "|3      |movie was alright but acting was horrible |[movie, was, alright, but, acting, was, horrible]  |\n",
      "|4      |I am never watching that movie ever again |[i, am, never, watching, that, movie, ever, again] |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal = StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|user_id|tokens                                             |refined_tokens                    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|1      |[i, really, liked, this, movie]                    |[really, liked, movie]            |\n",
      "|2      |[i, would, recommend, this, movie, to, my, friends]|[recommend, movie, friends]       |\n",
      "|3      |[movie, was, alright, but, acting, was, horrible]  |[movie, alright, acting, horrible]|\n",
      "|4      |[i, am, never, watching, that, movie, ever, again] |[never, watching, movie, ever]    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.select(['user_id','tokens','refined_tokens']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df=count_vec.fit(refined_df).transform(refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+---------------------------------+\n",
      "|user_id|refined_tokens                    |features                         |\n",
      "+-------+----------------------------------+---------------------------------+\n",
      "|1      |[really, liked, movie]            |(11,[0,2,8],[1.0,1.0,1.0])       |\n",
      "|2      |[recommend, movie, friends]       |(11,[0,4,9],[1.0,1.0,1.0])       |\n",
      "|3      |[movie, alright, acting, horrible]|(11,[0,6,7,10],[1.0,1.0,1.0,1.0])|\n",
      "|4      |[never, watching, movie, ever]    |(11,[0,1,3,5],[1.0,1.0,1.0,1.0]) |\n",
      "+-------+----------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_df.select(['user_id','refined_tokens','features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'ever',\n",
       " 'liked',\n",
       " 'friends',\n",
       " 'horrible',\n",
       " 'alright',\n",
       " 'acting',\n",
       " 'recommend',\n",
       " 'never',\n",
       " 'really',\n",
       " 'watching']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.fit(refined_df).vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF,IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_df=hashing_vec.transform(refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------------------+\n",
      "|user_id|refined_tokens                    |tf_features                                            |\n",
      "+-------+----------------------------------+-------------------------------------------------------+\n",
      "|1      |[really, liked, movie]            |(262144,[14,32675,155321],[1.0,1.0,1.0])               |\n",
      "|2      |[recommend, movie, friends]       |(262144,[129613,155321,222394],[1.0,1.0,1.0])          |\n",
      "|3      |[movie, alright, acting, horrible]|(262144,[80824,155321,236263,240286],[1.0,1.0,1.0,1.0])|\n",
      "|4      |[never, watching, movie, ever]    |(262144,[63139,155321,203802,245806],[1.0,1.0,1.0,1.0])|\n",
      "+-------+----------------------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashing_df.select(['user_id','refined_tokens','tf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec=IDF(inputCol='tf_features',outputCol='tf_idf_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df=tf_idf_vec.fit(hashing_df).transform(hashing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------+\n",
      "|user_id|tf_idf_features                                                                                     |\n",
      "+-------+----------------------------------------------------------------------------------------------------+\n",
      "|1      |(262144,[14,32675,155321],[0.9162907318741551,0.9162907318741551,0.0])                              |\n",
      "|2      |(262144,[129613,155321,222394],[0.9162907318741551,0.0,0.9162907318741551])                         |\n",
      "|3      |(262144,[80824,155321,236263,240286],[0.9162907318741551,0.0,0.9162907318741551,0.9162907318741551])|\n",
      "|4      |(262144,[63139,155321,203802,245806],[0.9162907318741551,0.0,0.9162907318741551,0.9162907318741551])|\n",
      "+-------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_df.select(['user_id','tf_idf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df=spark.read.csv('Movie_reviews.csv',inferSchema=True,header=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7087"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+---------+\n",
      "|Review                                                               |Sentiment|\n",
      "+---------------------------------------------------------------------+---------+\n",
      "|\"I love the Harry Potter series if you can count that as \"\" a \"\" book| also\"   |\n",
      "|we're gonna like watch Mission Impossible or Hoot.(                  |1        |\n",
      "|The Da Vinci Code is awesome!!                                       |1        |\n",
      "|by the way, the Da Vinci Code sucked, just letting you know...       |0        |\n",
      "|I hate Harry Potter.                                                 |0        |\n",
      "|Brokeback Mountain is a beautiful film..                             |1        |\n",
      "|The Da Vinci Code is awesome..                                       |1        |\n",
      "|A futile mission impossible ending before it even began.             |0        |\n",
      "|I LOVE THE DA VINCI CODE!!..                                         |1        |\n",
      "|I love Da Vinci Code.                                                |1        |\n",
      "+---------------------------------------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df=text_df.filter(((text_df.Sentiment =='1') | (text_df.Sentiment =='0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        0| 3081|\n",
      "|        1| 3909|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = text_df.withColumn(\"Label\", text_df.Sentiment.cast('float')).drop('Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----+\n",
      "|Review                                                                  |Label|\n",
      "+------------------------------------------------------------------------+-----+\n",
      "|I hate Harry Potter, it's retarted, gay and stupid and there's only one |0.0  |\n",
      "|Not because I hate Harry Potter, but because I am the type of person tha|0.0  |\n",
      "|Harry Potter's awesome..                                                |1.0  |\n",
      "|I love The Da Vinci Code...                                             |1.0  |\n",
      "|Da Vinci Code sucks.                                                    |0.0  |\n",
      "|i love kirsten / leah / kate escapades and mission impossible tom as wel|1.0  |\n",
      "|I love Harry Potter.                                                    |1.0  |\n",
      "|\"Anyway, thats why I love \"\" Brokeback Mountain.\"                       |1.0  |\n",
      "|I am going to start reading the Harry Potter series again because that i|1.0  |\n",
      "|Brokeback Mountain is fucking horrible..                                |0.0  |\n",
      "+------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 3909|\n",
      "|  0.0| 3081|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add length to the dataframe\n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df=text_df.withColumn('length',length(text_df['Review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----+------+\n",
      "|Review                                                                  |Label|length|\n",
      "+------------------------------------------------------------------------+-----+------+\n",
      "|friday hung out with kelsie and we went and saw The Da Vinci Code SUCKED|0.0  |72    |\n",
      "|Mission Impossible III sucks..                                          |0.0  |30    |\n",
      "|Because I like Harry Potter and I in Eragon's case, I like dragons...   |1.0  |69    |\n",
      "|Combining the opinion / review from Gary and Gin Zen, The Da Vinci Code |0.0  |71    |\n",
      "|The Da Vinci Code is awesome!!                                          |1.0  |30    |\n",
      "|Da Vinci Code = Up, Up, Down, Down, Left, Right, Left, Right, B, A, SUCK|0.0  |72    |\n",
      "|I love Brokeback Mountain....                                           |1.0  |29    |\n",
      "|MI: 2 was generally agreed to be a decent action movie but a terrible fi|0.0  |72    |\n",
      "|The Da Vinci Code is awesome!!                                          |1.0  |30    |\n",
      "|I am sick of Harry Potter shit-I am sure the kids must be too...        |0.0  |64    |\n",
      "+------------------------------------------------------------------------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Label|      avg(Length)|\n",
      "+-----+-----------------+\n",
      "|  1.0|47.61882834484523|\n",
      "|  0.0|50.95845504706264|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('Label').agg({'Length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Tokenize & StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='Review',outputCol='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df=tokenization.transform(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+\n",
      "|              Review|Label|length|              tokens|\n",
      "+--------------------+-----+------+--------------------+\n",
      "|The Da Vinci Code...|  1.0|    39|[the, da, vinci, ...|\n",
      "|this was the firs...|  1.0|    72|[this, was, the, ...|\n",
      "|i liked the Da Vi...|  1.0|    32|[i, liked, the, d...|\n",
      "|i liked the Da Vi...|  1.0|    32|[i, liked, the, d...|\n",
      "|I liked the Da Vi...|  1.0|    72|[i, liked, the, d...|\n",
      "|that's not even a...|  1.0|    72|[that's, not, eve...|\n",
      "|I loved the Da Vi...|  1.0|    72|[i, loved, the, d...|\n",
      "|i thought da vinc...|  1.0|    57|[i, thought, da, ...|\n",
      "|The Da Vinci Code...|  1.0|    45|[the, da, vinci, ...|\n",
      "|I thought the Da ...|  1.0|    51|[i, thought, the,...|\n",
      "|The Da Vinci Code...|  1.0|    68|[the, da, vinci, ...|\n",
      "|The Da Vinci Code...|  1.0|    62|[the, da, vinci, ...|\n",
      "|then I turn on th...|  1.0|    66|[then, i, turn, o...|\n",
      "|The Da Vinci Code...|  1.0|    34|[the, da, vinci, ...|\n",
      "|i love da vinci c...|  1.0|    24|[i, love, da, vin...|\n",
      "|i loved da vinci ...|  1.0|    23|[i, loved, da, vi...|\n",
      "|TO NIGHT:: THE DA...|  1.0|    52|[to, night::, the...|\n",
      "|THE DA VINCI CODE...|  1.0|    40|[the, da, vinci, ...|\n",
      "|Thing is, I enjoy...|  1.0|    38|[thing, is,, i, e...|\n",
      "|very da vinci cod...|  1.0|    38|[very, da, vinci,...|\n",
      "+--------------------+-----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_text_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+--------------------+\n",
      "|              Review|Label|length|              tokens|      refined_tokens|\n",
      "+--------------------+-----+------+--------------------+--------------------+\n",
      "|The Da Vinci Code...|  1.0|    39|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|this was the firs...|  1.0|    72|[this, was, the, ...|[first, clive, cu...|\n",
      "|i liked the Da Vi...|  1.0|    32|[i, liked, the, d...|[liked, da, vinci...|\n",
      "|i liked the Da Vi...|  1.0|    32|[i, liked, the, d...|[liked, da, vinci...|\n",
      "|I liked the Da Vi...|  1.0|    72|[i, liked, the, d...|[liked, da, vinci...|\n",
      "|that's not even a...|  1.0|    72|[that's, not, eve...|[even, exaggerati...|\n",
      "|I loved the Da Vi...|  1.0|    72|[i, loved, the, d...|[loved, da, vinci...|\n",
      "|i thought da vinc...|  1.0|    57|[i, thought, da, ...|[thought, da, vin...|\n",
      "|The Da Vinci Code...|  1.0|    45|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|I thought the Da ...|  1.0|    51|[i, thought, the,...|[thought, da, vin...|\n",
      "|The Da Vinci Code...|  1.0|    68|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|The Da Vinci Code...|  1.0|    62|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|then I turn on th...|  1.0|    66|[then, i, turn, o...|[turn, light, rad...|\n",
      "|The Da Vinci Code...|  1.0|    34|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|i love da vinci c...|  1.0|    24|[i, love, da, vin...|[love, da, vinci,...|\n",
      "|i loved da vinci ...|  1.0|    23|[i, loved, da, vi...|[loved, da, vinci...|\n",
      "|TO NIGHT:: THE DA...|  1.0|    52|[to, night::, the...|[night::, da, vin...|\n",
      "|THE DA VINCI CODE...|  1.0|    40|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|Thing is, I enjoy...|  1.0|    38|[thing, is,, i, e...|[thing, is,, enjo...|\n",
      "|very da vinci cod...|  1.0|    38|[very, da, vinci,...|[da, vinci, code,...|\n",
      "+--------------------+-----+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "\n",
    "refined_text_df = refined_text_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|              Review|Label|length|              tokens|      refined_tokens|token_count|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|Which is why i sa...|  1.0|    72|[which, is, why, ...|[said, silent, hi...|          8|\n",
      "|Mission Impossibl...|  1.0|    35|[mission, impossi...|[mission, impossi...|          4|\n",
      "|I hate Harry Potter.|  0.0|    20|[i, hate, harry, ...|[hate, harry, pot...|          3|\n",
      "|Brokeback Mountai...|  0.0|    40|[brokeback, mount...|[brokeback, mount...|          4|\n",
      "|I would like to k...|  0.0|    72|[i, would, like, ...|[like, kill, writ...|          7|\n",
      "|Because I would l...|  1.0|    72|[because, i, woul...|[like, make, frie...|          6|\n",
      "|These Harry Potte...|  0.0|    38|[these, harry, po...|[harry, potter, m...|          5|\n",
      "|I, too, like Harr...|  1.0|    27|[i,, too,, like, ...|[i,, too,, like, ...|          5|\n",
      "|Da Vinci Code suc...|  0.0|    22|[da, vinci, code,...|[da, vinci, code,...|          4|\n",
      "|the people who ar...|  1.0|    67|[the, people, who...|[people, worth, k...|          8|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the refined tokens after stopword removal, we can\n",
    "use any of the above approaches to convert text into numerical features.\n",
    "In this case, we use a **countvectorizer for feature vectorization** for the\n",
    "Machine Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_text_df=count_vec.fit(refined_text_df).transform(refined_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+-----+\n",
      "|      refined_tokens|token_count|            features|Label|\n",
      "+--------------------+-----------+--------------------+-----+\n",
      "|[da, vinci, code,...|          5|(2302,[0,1,4,43,2...|  1.0|\n",
      "|[first, clive, cu...|          9|(2302,[11,51,229,...|  1.0|\n",
      "|[liked, da, vinci...|          5|(2302,[0,1,4,53,3...|  1.0|\n",
      "|[liked, da, vinci...|          5|(2302,[0,1,4,53,3...|  1.0|\n",
      "|[liked, da, vinci...|          8|(2302,[0,1,4,53,6...|  1.0|\n",
      "|[even, exaggerati...|          6|(2302,[46,229,271...|  1.0|\n",
      "|[loved, da, vinci...|          8|(2302,[0,1,22,30,...|  1.0|\n",
      "|[thought, da, vin...|          7|(2302,[0,1,4,228,...|  1.0|\n",
      "|[da, vinci, code,...|          6|(2302,[0,1,4,33,2...|  1.0|\n",
      "|[thought, da, vin...|          7|(2302,[0,1,4,223,...|  1.0|\n",
      "+--------------------+-----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_text_df.select(['refined_tokens','token_count','features','Label']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data for building model\n",
    "model_text_df=cv_text_df.select(['features','token_count','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **VectorAssembler** to create input features for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['features','token_count'],outputCol='features_vec')\n",
    "model_text_df = df_assembler.transform(model_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- token_count: integer (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data \n",
    "training_df,test_df=model_text_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 2867|\n",
      "|  0.0| 2281|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 1042|\n",
      "|  0.0|  800|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any of the classification models on this data, but we\n",
    "proceed with training the **Logistic Regression Model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg=LogisticRegression(featuresCol='features_vec',labelCol='Label').fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=log_reg.evaluate(test_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|token_count|Label|        features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|(2302,[0,1,4,5,30...|          5|  1.0|(2303,[0,1,4,5,30...|[-22.938807722936...|[1.09094381499421...|       1.0|\n",
      "|(2302,[0,1,4,5,36...|          5|  1.0|(2303,[0,1,4,5,36...|[-20.637197101780...|[1.08988123771091...|       1.0|\n",
      "|(2302,[0,1,4,5,44...|          5|  1.0|(2303,[0,1,4,5,44...|[-20.829485407724...|[8.99227181577077...|       1.0|\n",
      "|(2302,[0,1,4,10,1...|         10|  0.0|(2303,[0,1,4,10,1...|[32.3989845368892...|[0.99999999999999...|       0.0|\n",
      "|(2302,[0,1,4,11,4...|          7|  1.0|(2303,[0,1,4,11,4...|[-23.082993660268...|[9.44459256677047...|       1.0|\n",
      "|(2302,[0,1,4,12,1...|         10|  1.0|(2303,[0,1,4,12,1...|[-25.612252762764...|[7.52904609485113...|       1.0|\n",
      "|(2302,[0,1,4,12,1...|          5|  1.0|(2303,[0,1,4,12,1...|[-18.237938010956...|[1.20050550347868...|       1.0|\n",
      "|(2302,[0,1,4,12,1...|          8|  1.0|(2303,[0,1,4,12,1...|[-17.549586003515...|[2.38952527508337...|       1.0|\n",
      "|(2302,[0,1,4,12,2...|          7|  1.0|(2303,[0,1,4,12,2...|[-19.080310132344...|[5.17042918422398...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-20.191395521079...|[1.70211281913048...|       1.0|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "true_postives = results[(results.Label == 1) & (results.prediction == 1)].count()\n",
    "true_negatives = results[(results.Label == 0) & (results.prediction == 0)].count()\n",
    "false_positives = results[(results.Label == 0) & (results.prediction == 1)].count()\n",
    "false_negatives = results[(results.Label == 1) & (results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9875239923224568\n"
     ]
    }
   ],
   "source": [
    "recall = float(true_postives)/(true_postives + false_negatives)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9716713881019831\n"
     ]
    }
   ],
   "source": [
    "precision = float(true_postives) / (true_postives + false_positives)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9766558089033659\n"
     ]
    }
   ],
   "source": [
    "accuracy=float((true_postives+true_negatives) /(results.count()))\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
